{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\n",
    "    \"hwaseem04/Aya-testing\",\n",
    "    data_files={\"xm3600_captioning\": \"data/xm3600_captioning-00000-of-00001.parquet\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['xm3600_captioning'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "# Load Gemma model and processor\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\").eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Languages to iterate over\n",
    "languages = [\"en\", \"bn\", \"de\", \"ko\", \"ru\", \"zh\"]\n",
    "\n",
    "# Directory to save temp images (needed for this model)\n",
    "os.makedirs(\"temp_images_caption\", exist_ok=True)\n",
    "\n",
    "dataset = ds['xm3600_captioning']\n",
    "\n",
    "for sample in tqdm(dataset, desc=\"Iterating samples\"):\n",
    "    try:\n",
    "        image = sample[\"image\"]\n",
    "        sample_id = sample[\"sample_id\"]\n",
    "\n",
    "        # Save image locally if it doesn't already exist\n",
    "        image_path = f\"temp_images_caption/{sample_id}.jpg\"\n",
    "        if not os.path.exists(image_path):\n",
    "            image.save(image_path)\n",
    "\n",
    "        print(f\"\\n========== Sample ID: {sample_id} ==========\")\n",
    "\n",
    "        for lang in languages:\n",
    "            prompt_col = f\"prompt_{lang}\"\n",
    "            caption_col = f\"captions_{lang}\"\n",
    "\n",
    "            # Skip if required fields are missing\n",
    "            if prompt_col not in sample or caption_col not in sample:\n",
    "                print(f\"[{lang}] Missing data.\")\n",
    "                continue\n",
    "\n",
    "            prompt = sample[prompt_col]\n",
    "            gt_caption = sample[caption_col]\n",
    "\n",
    "            # Build the prompt using Gemma's chat template style\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image_path},\n",
    "                        {\"type\": \"text\", \"text\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Prepare input using Gemma's processor\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "            input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "            # Generate output from model\n",
    "            with torch.inference_mode():\n",
    "                generation = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                generation = generation[0][input_len:]\n",
    "\n",
    "            # Decode the generated tokens\n",
    "            pred_caption = processor.decode(generation, skip_special_tokens=True)\n",
    "\n",
    "            # Display results for this language\n",
    "            print(f\"\\n[{lang.upper()}]\")\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"GT: {gt_caption}\")\n",
    "            print(f\"Pred: {textwrap.fill(pred_caption, width=80)}\")\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {sample['sample_id']}: {e}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
